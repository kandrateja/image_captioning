# -*- coding: utf-8 -*-
"""Image_Captioning_attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bVMW58kBT0nMp7Rsdl6TGNYyqAX1zUBt
"""

from google.colab import drive
drive.mount("/content/drive")

import string
from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt
import keras
import sys, time, os, warnings
warnings.filterwarnings("ignore")
import re
import numpy as np
import pandas as pd
from PIL import Image
import pickle
from collections import Counter
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense, BatchNormalization
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import load_img, img_to_array
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

"""### Utility functions"""

# load doc into memory
def load_doc(filename):
    # open the file as read only
    file = open(filename, 'r')
    # read all text
    text = file.read()
    # close the file
    file.close()
    return text

# To remove punctuations
def remove_punctuation(text_original):
    text_no_punctuation = text_original.translate(string.punctuation)
    return(text_no_punctuation)

# To remove single characters
def remove_single_character(text):
    
    text_len_more_than1 = ""
    for word in text.split():
        
        if len(word) > 1:
            text_len_more_than1 += " " + word
    return(text_len_more_than1)

# To remove numeric values
def remove_numeric(text,printTF=False): 
    text_no_numeric = ""
    for word in text.split(): 
        isalpha = word.isalpha()
        if printTF:
            
            print(" {:10} : {:}".format(word,isalpha))
        if isalpha:
            
            text_no_numeric += " " + word
    return(text_no_numeric)

"""### Exploratory data analysis"""

import os
import string
os.getcwd()

import os
arr = os.listdir('/content/drive/MyDrive/Attention/Flickr_Data/Flickr_TextData')
print(arr)

"""* in this directory flickr8k_text we have captions related information for all the train/test/cv images"""

from os import listdir
## The location of the Flickr8K_ photos
image_dir = '/content/drive/MyDrive/Attention/Flickr_Data/Images'
images = listdir(image_dir)
## The location of the caption file
descriptions_dir = '/content/drive/MyDrive/Attention/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'
print("The number of jpg flies in Flicker8k: {}".format(len(images)))

doc = load_doc(descriptions_dir)
print(doc[:400])

"""### see here in the description_dir has image id and captions for that image

* typically from the above we can observe that we have 4 captions(references) for the single image(1000268201_693b08cb0e.jpg)
"""

def make_dataset(text):
    
    df = []
    for sentences in text.split('\n'):
        splitted = sentences.split('\t')
        if len(splitted) == 1:
            continue
        w = splitted[0].split("#")
        df.append(w + [splitted[1].lower()])
    return df

df = make_dataset(doc)
df[:2]

"""### see here we're  reconstructing the information like image_id, index number(0 to 4) and the caption related to it"""

data = pd.DataFrame(df,columns=["filename","index","caption"])
# Reordering columns for better readability
data = data.reindex(columns =['index','filename','caption'])
data.head()

# If any filename doesn't have .jpg extension at last then mark it as Invalid filename
def invalid_filename_check(data):
    
    for filenames in data["filename"]:
        found = re.search("(.(jpg)$)", filenames)
        if (found):
            pass
        else:
            print("Error file: {}".format(filenames))

#remove these kind of files
invalid_filename_check(data)

#removing those files
data= data[data['filename']!= '2258277193_586949ec62.jpg.1']
data.shape

# from this we can say that we have exactly 8091 images and each of those image have 5 captions each
40455/5

def utility_counter(data):

    unique_filenames = np.unique(data.filename.values)
    print("The number of unique file names : {}".format(len(unique_filenames)))

    print("The distribution of the number of captions for each image:")
    ct = Counter(Counter(data.filename.values).values())
    print(ct)
    return unique_filenames

unique_filenames = utility_counter(data)

#Function to plot the images and its description
# https://fairyonice.github.io/Develop_an_image_captioning_deep_learning_model_using_Flickr_8K_data.html
def image_desc_plotter(data):
    
    npic = 5
    npix = 224
    target_size = (npix,npix,3)

    count = 1
    fig = plt.figure(figsize=(10,20))
    for jpgfnm in unique_filenames[2:5]:
        
        
        filename = image_dir + '/' + jpgfnm
        captions = list(data["caption"].loc[data["filename"]==jpgfnm].values)
        image_load = load_img(filename, target_size=target_size)

        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])
        ax.imshow(image_load)
        count += 1

        ax = fig.add_subplot(npic,2,count)
        plt.axis('off')
        ax.plot()
        ax.set_xlim(0,1)
        ax.set_ylim(0,len(captions))
        for i, caption in enumerate(captions):
            ax.text(0,i,caption,fontsize=20)
        count += 1
    plt.show()

image_desc_plotter(data)

def create_vocabulary(data):
    
    vocab = []
    for captions in data.caption.values:
        vocab.extend(captions.split())
    print("Vocabulary Size : {}".format(len(set(vocab))))
    return vocab

vocabulary = create_vocabulary(data)

# this code is all about the frequency of the word count 
def df_word_count(data,vocabulary):
    ct = Counter(vocabulary)
    appen_1 = []
    appen_2 = []
    for i in ct.keys():
        appen_1.append(i)
    for j in ct.values():
        appen_2.append(j)
    data = {"word":appen_1 , "count":appen_2}
    dfword = pd.DataFrame(data)
    dfword = dfword.sort_values(by='count', ascending=False)
    dfword = dfword.reset_index()[["word","count"]]
    return(dfword)

dfwordcount = df_word_count(data,vocabulary)

#frequency of the word
dfwordcount.iloc[:5,:]

topn = 50

def plthist(dfsub, title="The top 50 most frequently appearing words"):
    plt.figure(figsize=(30,3))
    plt.bar(dfsub.index,dfsub["count"],color ='g')
    plt.yticks(fontsize=20,color ='r')
    plt.xticks(dfsub.index,dfsub["word"],rotation=90,fontsize=20,color ='r')
    plt.title(title,fontsize=20)
    plt.show()

plthist(dfwordcount.iloc[:topn,:],
        title="The top 50 most frequently appearing words")
plthist(dfwordcount.iloc[-topn:,:],
        title="The least 50 most frequently appearing words")

"""### removing the unneccesary text in the captions"""

def text_clean(text_original):
    text = remove_punctuation(text_original)
    text = remove_single_character(text)
    text = remove_numeric(text)
    return(text)
    
for i, caption in enumerate(data.caption.values):
    newcaption = text_clean(caption)
    data["caption"].iloc[i] = newcaption

"""### Now check the vocabulary size"""

#as expected, size of the vocabulary is decreased
clean_vocabulary = create_vocabulary(data)

"""### now printing the top words/least words"""

dfwordcount = df_word_count(data,clean_vocabulary)
plthist(dfwordcount.iloc[:topn,:],
        title="The top 50 most frequently appearing words")
plthist(dfwordcount.iloc[-topn:,:],
        title="The least 50 most frequently appearing words")

"""### Here we're appending each image path to the list for the later use"""

def preprocess_images(data):
    
    all_img_name_vector = []

    for filenames in data["filename"]:
         full_image_path = image_dir+"/"+ filenames
         all_img_name_vector.append(full_image_path)
    return all_img_name_vector
all_img_name_vector = preprocess_images(data)
all_img_name_vector[:10]

def preprocess_captions(data):
    
    total_captions = []

    for caption  in data["caption"].astype(str):
        caption = '<starting> ' + caption+ ' <ended>'
        total_captions.append(caption)
    return total_captions
total_captions = preprocess_captions(data)
total_captions[:10]

print("Total Images : " + str(len(all_img_name_vector)))
print("Total Captions : " + str(len(total_captions)))

"""### Here we will select only 40000 images only in this way we can process the images via 625 batches and each batch has 64 images"""

def data_limiter(num,total_captions,all_img_name_vector):
    # Shuffle captions and image_names together
    train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)
    train_captions = train_captions[:num]
    img_name_vector = img_name_vector[:num]
    return train_captions,img_name_vector

train_captions,img_name_vector = data_limiter(40000,total_captions,all_img_name_vector)

print("Total Captions = {0} , Total images = {1}".format(len(train_captions),len(img_name_vector)))

from pickle import dump

# save to file
dump(train_captions, open('/content/drive/MyDrive/Attention/captions.pkl', 'wb'))
dump(img_name_vector, open('/content/drive/MyDrive/Attention/img_names.pkl', 'wb'))

train_captions = load(open('/content/drive/MyDrive/Attention/captions.pkl', 'rb'))
img_name_vector = load(open('/content/drive/MyDrive/Attention/img_names.pkl', 'rb'))

# To know the shape of images
def img_shape_finder(image):
    img= plt.imread(image)

    print("Shape of the image ==> {0} is ==> {1}".format(image,img.shape))

img_list=[]
for i in range(10):
    img_list.append(img_name_vector[i])

for j in img_list:
    img_shape_finder(j)

# To know the shape of images
def image_and_shapes(image):
    img= plt.imread(image)
    plt.imshow(img)
    print("Shape of the image:{}".format(img.shape))

image_and_shapes("/content/drive/MyDrive/Attention/Flickr_Data/Images/2831314869_5025300133.jpg")

import imageio
import tensorflow as tf
def image_flipper(image):
    original_img = imageio.imread(image)

    plt.figure(1)

#Original Image

    plt.subplot(221)
    plt.imshow(original_img)

    #Left-Right flip Image

    flipped_img_tensor = tf.image.flip_left_right(original_img)
    flipped_img= flipped_img_tensor.numpy()
    plt.subplot(222)
    plt.imshow(flipped_img)

    #Up-Down flip Image

    upside_down_flip_tensor = tf.image.flip_up_down(original_img)
    upside_down_flip= upside_down_flip_tensor.numpy()
    plt.subplot(223)
    plt.imshow(upside_down_flip)

    #Gray scale Image

    gray_tensor = tf.image.rgb_to_grayscale(original_img)
    grayimg= gray_tensor.numpy()
    plt.subplot(224)
    plt.imshow(tf.squeeze(grayimg))

image_flipper("/content/drive/MyDrive/Attention/Flickr_Data/Images/2831314869_5025300133.jpg")

import tensorflow as tf
def load_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (299, 299))
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    return img, image_path

img1,img1_path = load_image("/content/drive/MyDrive/Attention/Flickr_Data/Images/319869052_08b000e4af.jpg")
print("Shape after resize :", img1.shape)
plt.imshow(img1)

"""Using inceptionv3 encoding type"""

image_model = tf.keras.applications.InceptionV3(include_top=False,
                                                weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output

image_features_extract_model = tf.keras.Model(new_input, hidden_layer)

image_features_extract_model.summary()

# Get unique images
encode_train = sorted(set(img_name_vector))
image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)

image_dataset

# Feel free to change batch_size according to your system configuration
image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)

image_dataset

from tqdm import tqdm
for img, path in tqdm(image_dataset):
    batch_features = image_features_extract_model(img)
    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))
    for bf, p in zip(batch_features, path):
        path_of_feature = p.numpy().decode("utf-8")
        np.save(path_of_feature, bf.numpy())

def tokenize_caption(top_k,train_captions):
    # Choose the top 5000 words from the vocabulary
    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,oov_token="<unk>",filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
    # oov_token: if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls

    tokenizer.fit_on_texts(train_captions)
    train_seqs = tokenizer.texts_to_sequences(train_captions)

    # Map '<pad>' to '0'
    tokenizer.word_index['<pad>'] = 0
    tokenizer.index_word[0] = '<pad>'


    # Create the tokenized vectors
    train_seqs = tokenizer.texts_to_sequences(train_captions)
    return train_seqs, tokenizer

train_seqs , tokenizer = tokenize_caption(5000,train_captions)

train_captions[:3]

train_seqs[:3]

len(tokenizer.word_counts)

8918-8326# this many out of vacabulary words are there

# Find the maximum length of any caption in our dataset
def calc_max_length(tensor):
    return max(len(t) for t in tensor)
# Calculates the max_length, which is used to store the attention weights
max_length = calc_max_length(train_seqs)

# Find the maximum length of any caption in our dataset
def calc_min_length(tensor):
    return min(len(t) for t in tensor)
# Calculates the max_length, which is used to store the attention weights
min_length = calc_min_length(train_seqs)

print('Max Length of any caption : Min Length of any caption = '+ str(max_length) +" : "+str(min_length))

def padding_train_sequences(train_seqs,max_length,padding_type):
    cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding=padding_type,maxlen=max_length)
    return cap_vector

padded_caption_vector = padding_train_sequences(train_seqs,max_length,'post')
print(padded_caption_vector.shape)

padded_caption_vector

"""## spliting train and test"""

# Create training and test set using an 80-20 split
img_name_train, img_name_test, caption_train, caption_test = train_test_split(img_name_vector,padded_caption_vector,test_size=0.2,random_state=0)

print("Training Data : X = {0},Y = {1}".format(len(img_name_train), len(caption_train)))
print("Test Data : X = {0},Y = {1}".format(len(img_name_test), len(caption_test)))

def load_npy(img_name, cap):
    img_tensor = np.load(img_name.decode('utf-8')+'.npy')
    return img_tensor, cap

"""### loading the images batch wise"""

def create_dataset(img_name_train,caption_train):

    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, caption_train))

    # Use map to load the numpy files in parallel
    dataset = dataset.map(lambda item1, item2: tf.numpy_function(load_npy, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)

    # Shuffle and batch
    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    return dataset

BATCH_SIZE = 64
BUFFER_SIZE = 1000

# Creating train and test dataset
dataset = create_dataset(img_name_train,caption_train)
test_dataset = create_dataset(img_name_test,caption_test)

# Feel free to change these parameters according to your system's configuration

embedding_dim = 256
units = 512
vocab_size = 8357 + 1
num_steps= len(img_name_train) // BATCH_SIZE

# Shape of the vector extracted from InceptionV3 is (64, 2048)
# These two variables represent that vector shape
features_shape = 2048
attention_features_shape = 64

num_steps_train

# https://www.tensorflow.org/tutorials/text/image_captioning
class CNN_Encoder(tf.keras.Model):
    # Since you have already extracted the features and dumped it using pickle
    # This encoder passes those features through a Fully connected layer
    def __init__(self, embedding_dim):
        super(CNN_Encoder, self).__init__()
        # shape after fc == (batch_size, 64, embedding_dim)
        self.fc = tf.keras.layers.Dense(embedding_dim)

    def call(self, x):
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x

"""#### RNN DECODER BahdanauAttention






"""

class BahdanauAttention(tf.keras.Model):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, features, hidden):
    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)

    # hidden shape == (batch_size, hidden_size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)
    hidden_with_time_axis = tf.expand_dims(hidden, 1)

    # attention_hidden_layer shape == (batch_size, 64, units)
    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +
                                         self.W2(hidden_with_time_axis)))

    # score shape == (batch_size, 64, 1)
    # This gives you an unnormalized score for each image feature.
    score = self.V(attention_hidden_layer)

    # attention_weights shape == (batch_size, 64, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * features
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

class RNN_Decoder(tf.keras.Model):
  def __init__(self, embedding_dim, units, vocab_size):
    super(RNN_Decoder, self).__init__()
    self.units = units

    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer=tf.keras.initializers.HeNormal)
    self.fc1 = tf.keras.layers.Dense(self.units)
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    self.attention = BahdanauAttention(self.units)

  def call(self, x, features, hidden):
    # defining attention as a separate model
    context_vector, attention_weights = self.attention(features, hidden)

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # shape == (batch_size, max_length, hidden_size)
    x = self.fc1(output)

    # x shape == (batch_size * max_length, hidden_size)
    x = tf.reshape(x, (-1, x.shape[2]))

    # output shape == (batch_size * max_length, vocab)
    x = self.fc2(x)

    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))

encoder = CNN_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask

    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)

checkpoint_path_ckpt = "train"
ckpt = tf.train.Checkpoint(encoder=encoder,
                           decoder=decoder,
                           optimizer = optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)

start_epoch = 0
if ckpt_manager.latest_checkpoint:
    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])

# Clear previous logs
!rm -rf '/content/train'

# Define our metrics
train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)

import datetime
train_log_dir  =os.path.join("logs",datetime.datetime.now().strftime("%m/%d/%Y, %H:%M:%S"))
print(train_log_dir)
test_log_dir =os.path.join("logs",datetime.datetime.now().strftime("%m/%d/%Y, %H:%M:%S"))
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
test_summary_writer = tf.summary.create_file_writer(test_log_dir)

@tf.function
def train_step(img_tensor, target):
  loss = 0

  # initializing the hidden state for each batch
  # because the captions are not related from image to image
  hidden = decoder.reset_state(batch_size=target.shape[0])

  dec_input = tf.expand_dims([tokenizer.word_index['<starting>']] * target.shape[0], 1)

  with tf.GradientTape() as tape:
      features = encoder(img_tensor)

      for i in range(1, target.shape[1]):
          # passing the features through the decoder
          predictions, hidden, _ = decoder(dec_input, features, hidden)

          loss += loss_function(target[:, i], predictions)

          # using teacher forcing
          dec_input = tf.expand_dims(target[:, i], 1)

  

  trainable_variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, trainable_variables)

  optimizer.apply_gradients(zip(gradients, trainable_variables))

  return loss,gradients

test_loss_plot = []

@tf.function
def test_step(img_tensor, target):
    loss = 0

    # initializing the hidden state for each batch
    # because the captions are not related from image to image
    hidden = decoder.reset_state(batch_size=target.shape[0])

    dec_input = tf.expand_dims([tokenizer.word_index['<starting>']] * BATCH_SIZE, 1)

    features = encoder(img_tensor)

    for i in range(1, target.shape[1]):
            
      # passing the features through the decoder

        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)
      # predictions : (64,8329)
        loss += loss_function(target[:, i], predictions)

        predicted_id = tf.argmax(predictions[0])
        dec_input = tf.expand_dims([predicted_id]*BATCH_SIZE, 1)
   

    return loss

loss_plot=[]
for epoch in range(start_epoch, 20):
    
    start = time.time()

    #For Train
    #================================================================
    total_loss_train = 0
    for (batch, (img_tensor, target)) in enumerate(dataset):
        
      t_loss,grads = train_step(img_tensor, target)
      
      total_loss_train += t_loss
    
    # storing the epoch end loss value to plot later
    loss_plot.append(total_loss_train / num_steps)

    # Tensorboard
    with train_summary_writer.as_default():
        
        tf.summary.scalar('LossPlotTrain', (total_loss_train/ num_steps), step=epoch)
        for index, grad in enumerate(grads):

          tf.summary.histogram('grads',grads[index],step=epoch)
   


    #For Test
    #================================================================
    total_loss_test = 0
    for (batch, (img_tensor, target)) in enumerate(test_dataset):
        t_loss = test_step(img_tensor, target)
        total_loss_test += t_loss
    # storing the epoch end loss value to plot later
    test_loss_plot.append(total_loss_test / num_steps) 
     # Tensorboard
    with test_summary_writer.as_default():
        
        tf.summary.scalar('LossPlotTest', (total_loss_test/ num_steps), step=epoch)
    # Get the gradient pairs (Tensor, Variable) 
 

    
    if epoch % 5 == 0:
        ckpt_manager.save()



    print ('Epoch {} TrainLoss {:.6f} TestLoss {:.6f}'.format(epoch + 1,(total_loss_train/num_steps),(total_loss_test/num_steps)))
    print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

from google.colab import drive
drive.mount('/content/drive')

import os
os.getcwd()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs

"""### Evaluating the Captioning Model:

* The evaluate function is similar to the training loop, except we don't use Teacher Forcing here. The input to Decoder at each time step is its previous predictions, along with the hidden state and the ENCODER output.

* Few key points to remember while making predictions.
- Stop predicting when the model predicts the end token.
- Store the attention weights for every time step.

**Understandings of attention plot**

See here firstly we are passing the image to the input and later we're extracting the features based on it and then we input to the decoder(features +start word) then it will return the attention weights + predicted word, so in the second time step we will pass the features+previous predicted words so then it will predict the next word in this way we will stop the prediction after seeing the stop word or max length of the sequence

while plotting the attention we already have the attention weights for each word, so based on each word attention weights we will plot the images

### Greedy Search
"""

def evaluate(image):
    attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder.reset_state(batch_size=1)

    temp_input = tf.expand_dims(load_image(image)[0], 0)
   
    img_tensor_val = image_features_extract_model(temp_input)
    
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<starting>']], 0) # Start token is <start> for LSTM
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)

        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.argmax(predictions[0]).numpy()
        result.append(tokenizer.index_word[predicted_id]) # Store the result or word predicted

        if tokenizer.index_word[predicted_id] == '<ended>': # if you reach <end> token then return the result
            return result, attention_plot

        dec_input = tf.expand_dims([predicted_id], 0) # Decoder input is the word predicted at previous timestep

    attention_plot = attention_plot[:len(result), :]
    return result, attention_plot

def plot_attention(image, result, attention_plot):
    temp_image = np.array(Image.open(image))

    fig = plt.figure(figsize=(10, 10))
    len_result = len(result)
    for l in range(len_result):
        temp_att = np.resize(attention_plot[l], (8, 8))
        ax = fig.add_subplot(len_result//2, len_result//2, l+1)
        ax.set_title(result[l])
        img = ax.imshow(temp_image)
        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())

    plt.tight_layout()
    plt.show()

# Below file conatains the names of images to be used in validation data
dev_images_file = '/content/drive/MyDrive/Attention/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt'
# Read the validation image names in a set# Read the validation image names in a set
dev_images = set(open(dev_images_file, 'r').read().strip().split('\n'))

type(dev_images)

# captions on the validation set

from PIL import Image
import seaborn as sns
from nltk.translate.bleu_score import sentence_bleu

scores=[]
start = time.time()
count=0
for element in dev_images:
  count+=1
  full_image_path = image_dir+"/"+ element
  image = full_image_path
  
  print(image)
  
  row =data[data['filename'] == element] 
  caption=row['caption']
  real_caption =caption.iloc[1]
  print("real caption....",real_caption)
  result, attention_plot = evaluate(image)

  # result is in this format:  ['woman', 'plays', 'volleyball', '<end>']
  # We need to remove <end> and join the list elements into a sentence. Also we need to remove <unk> from real_captions



  #remove "<unk>" in result
  for i in result:
      if i=="<unk>":
          result.remove(i)


  #remove <end> from result         
  result_join = ' '.join(result)
  result_final = result_join.rsplit(' ', 1)[0]

  real_appn = []
  real_appn.append(real_caption.split())
  reference = real_appn
  candidate = result_final.split()

  score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0.5, 0))
  scores.append(score*100)
  print(f"BELU score: {score*100}")

  print ('Real Caption:', real_caption)
  print ('Prediction Caption:', result_final)

  plot_attention(image, result, attention_plot)
  if count==6:
    break;

print(f"time took to Predict: {round(time.time()-start)} sec")

print("***********************************************************************")

ax = sns.boxplot(x=scores)
ax.set_title('Box blue scores')
ax.set_xlabel('scores range')

"""Train data blue score box plot"""

train_images_file = '/content/drive/MyDrive/Attention/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'
# Read the validation image names in a set# Read the test image names in a set
train_images = set(open(train_images_file, 'r').read().strip().split('\n'))

#box plot on train images
from PIL import Image
import seaborn as sns
from nltk.translate.bleu_score import sentence_bleu
scores=[]
start = time.time()
count=0
for element in train_images:
  count+=1
  
  full_image_path = image_dir+"/"+ element
  image = full_image_path
  row =data[data['filename'] == element] 
  caption=row['caption']
  real_caption =caption.iloc[1]
  result, attention_plot = evaluate(image)
  

  # result is in this format:  ['woman', 'plays', 'volleyball', '<end>']
  # We need to remove <end> and join the list elements into a sentence. Also we need to remove <unk> from real_captions


  

  #remove "<unk>" in result
  for i in result:
      if i=="<unk>":
          result.remove(i)


  #remove <end> from result         
  result_join = ' '.join(result)
  result_final = result_join.rsplit(' ', 1)[0]

  real_appn = []
  real_appn.append(real_caption.split())
  reference = real_appn
  candidate = result_final.split()

  score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))
  scores.append(score*100)
  if count==200:
    break;

ax = sns.boxplot(x=scores)
ax.set_title('Box blue scores')
ax.set_xlabel('scores range')

"""Test data blue score box plots"""

# Below file conatains the names of images to be used in test data
test_images_file = '/content/drive/MyDrive/Attention/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt'
# Read the validation image names in a set# Read the test image names in a set
test_images = set(open(test_images_file, 'r').read().strip().split('\n'))

scores=[]
start = time.time()
count=0
for element in test_images:
  count+=1
  
  full_image_path = image_dir+"/"+ element
  image = full_image_path
  row =data[data['filename'] == element] 
  caption=row['caption']
  real_caption =caption.iloc[1]
  result, attention_plot = evaluate(image)
  

  # result is in this format:  ['woman', 'plays', 'volleyball', '<end>']
  # We need to remove <end> and join the list elements into a sentence. Also we need to remove <unk> from real_captions


  

  #remove "<unk>" in result
  for i in result:
      if i=="<unk>":
          result.remove(i)


  #remove <end> from result         
  result_join = ' '.join(result)
  result_final = result_join.rsplit(' ', 1)[0]

  real_appn = []
  real_appn.append(real_caption.split())
  reference = real_appn
  candidate = result_final.split()

  score = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))
  scores.append(score*100)
  if count==200:
    break;

ax = sns.boxplot(x=scores)
ax.set_title('Box blue scores')
ax.set_xlabel('scores range')